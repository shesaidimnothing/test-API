# ğŸš€ DÃ©marrage Rapide - Tunnel Ollama

## âš¡ **Solution en 5 minutes**

### 1. **Installer ngrok**
```bash
# TÃ©lÃ©charger depuis https://ngrok.com/download
# Ou via npm
npm install -g ngrok
```

### 2. **Configurer Ollama**
```bash
# Windows (PowerShell en admin)
$env:OLLAMA_HOST="0.0.0.0:11434"
ollama serve

# Linux/Mac
export OLLAMA_HOST="0.0.0.0:11434"
ollama serve
```

### 3. **CrÃ©er le tunnel**
```bash
# Dans un nouveau terminal
ngrok http 11434 --host-header=localhost:11434
```

### 4. **Copier l'URL**
Vous verrez quelque chose comme :
```
Forwarding    https://abc123.ngrok.io -> http://localhost:11434
```
**Copiez `https://abc123.ngrok.io`**

### 5. **Configurer Vercel**
1. Allez sur votre projet Vercel
2. Settings > Environment Variables
3. Ajoutez :
   - **Name** : `OLLAMA_TUNNEL_URL`
   - **Value** : `https://abc123.ngrok.io`
4. RedÃ©ployez

### 6. **Tester**
Votre IA Ollama fonctionne maintenant sur Vercel ! ğŸ‰

## ğŸ”„ **Scripts Automatiques**

### Windows
```powershell
# ExÃ©cuter en tant qu'administrateur
.\start-ollama-tunnel.ps1
```

### Linux/Mac
```bash
./start-ollama-tunnel.sh
```

## ğŸ› **ProblÃ¨mes Courants**

### "Ollama n'est pas accessible"
- VÃ©rifiez que Ollama est dÃ©marrÃ©
- VÃ©rifiez la configuration `OLLAMA_HOST="0.0.0.0:11434"`

### "Tunnel ne fonctionne pas"
- VÃ©rifiez que ngrok est installÃ©
- VÃ©rifiez que le port 11434 est libre

### "Erreur sur Vercel"
- VÃ©rifiez la variable d'environnement `OLLAMA_TUNNEL_URL`
- VÃ©rifiez que le tunnel est actif

## ğŸ’¡ **Conseils**

- **Gardez votre PC allumÃ©** quand vous voulez utiliser l'IA
- **L'URL ngrok change** Ã  chaque redÃ©marrage (sauf compte payant)
- **Testez d'abord** avec `curl https://your-url.ngrok.io/api/tags`

## ğŸ¯ **RÃ©sultat**

Votre application Vercel appellera maintenant directement votre Ollama local ! ğŸš€
