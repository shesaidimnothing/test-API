# Configuration Tunnel Ollama pour Vercel

## üöÄ **Solution : Tunnel vers votre PC**

Cette solution permet √† Vercel d'appeler directement votre Ollama local via un tunnel.

## üîß **Option 1 : ngrok (Recommand√©e)**

### Installation
```bash
# T√©l√©charger ngrok depuis https://ngrok.com
# Ou via npm
npm install -g ngrok
```

### Configuration
```bash
# 1. Cr√©er un compte gratuit sur ngrok.com
# 2. Obtenir votre authtoken
ngrok config add-authtoken YOUR_AUTHTOKEN

# 3. Cr√©er un tunnel vers Ollama
ngrok http 11434 --host-header=localhost:11434
```

### Obtenir l'URL du tunnel
Apr√®s avoir lanc√© ngrok, vous verrez :
```
Session Status                online
Account                       your-email@example.com
Version                       3.x.x
Region                        Europe (eu)
Latency                       -
Web Interface                 http://127.0.0.1:4040
Forwarding                    https://abc123.ngrok.io -> http://localhost:11434
```

**Copiez l'URL `https://abc123.ngrok.io`**

### Configuration Vercel
1. **Variables d'environnement** sur Vercel :
   - Nom : `OLLAMA_TUNNEL_URL`
   - Valeur : `https://abc123.ngrok.io`

2. **Red√©ployer** l'application

## üîß **Option 2 : Cloudflare Tunnel (Gratuit)**

### Installation
```bash
# T√©l√©charger cloudflared depuis https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation/
```

### Configuration
```bash
# 1. Se connecter √† Cloudflare
cloudflared tunnel login

# 2. Cr√©er un tunnel
cloudflared tunnel create ollama-tunnel

# 3. Configurer le tunnel
cloudflared tunnel route dns ollama-tunnel ollama.yourdomain.com

# 4. D√©marrer le tunnel
cloudflared tunnel run ollama-tunnel
```

## üîß **Option 3 : localtunnel (Simple)**

### Installation
```bash
npm install -g localtunnel
```

### Configuration
```bash
# Cr√©er un tunnel
lt --port 11434 --subdomain your-ollama-tunnel
```

**URL g√©n√©r√©e :** `https://your-ollama-tunnel.loca.lt`

## üîß **Option 4 : Serveo (Sans installation)**

### Configuration
```bash
# Cr√©er un tunnel SSH
ssh -R 80:localhost:11434 serveo.net
```

**URL g√©n√©r√©e :** `https://serveo.net` (ou URL personnalis√©e)

## ‚öôÔ∏è **Configuration Ollama pour le Tunnel**

### Modifier la configuration Ollama
```bash
# Arr√™ter Ollama
# Modifier la configuration pour accepter les connexions externes

# Windows (PowerShell en admin)
$env:OLLAMA_HOST="0.0.0.0:11434"
ollama serve

# Linux/Mac
export OLLAMA_HOST="0.0.0.0:11434"
ollama serve
```

### V√©rifier la configuration
```bash
# Tester l'API via le tunnel
curl https://your-tunnel-url.ngrok.io/api/tags
```

## üéØ **Configuration Vercel**

### Variables d'environnement
```
OLLAMA_TUNNEL_URL=https://your-tunnel-url.ngrok.io
```

### Ordre de priorit√©
1. **Tunnel Ollama** (votre PC)
2. **API Externe** (OpenAI/Anthropic/HuggingFace)
3. **Mode D√©mo** (fallback)

## üîÑ **Script de D√©marrage Automatique**

### Windows (PowerShell)
```powershell
# start-ollama-tunnel.ps1
$env:OLLAMA_HOST="0.0.0.0:11434"
Start-Process -FilePath "ollama" -ArgumentList "serve" -WindowStyle Hidden
Start-Sleep -Seconds 5
Start-Process -FilePath "ngrok" -ArgumentList "http", "11434", "--host-header=localhost:11434"
```

### Linux/Mac
```bash
#!/bin/bash
# start-ollama-tunnel.sh
export OLLAMA_HOST="0.0.0.0:11434"
ollama serve &
sleep 5
ngrok http 11434 --host-header=localhost:11434
```

## üêõ **D√©pannage**

### Tunnel ne fonctionne pas
```bash
# V√©rifier que Ollama est accessible
curl http://localhost:11434/api/tags

# V√©rifier le tunnel
curl https://your-tunnel-url/api/tags
```

### Erreur CORS
```bash
# Ollama g√®re d√©j√† CORS, mais si probl√®me :
# V√©rifier que OLLAMA_ORIGINS contient votre domaine Vercel
```

### Tunnel se ferme
```bash
# ngrok gratuit se ferme apr√®s 2h
# Solution : Script de red√©marrage automatique
# Ou utiliser un compte payant ngrok
```

## üí° **Conseils**

1. **Utilisez ngrok** pour commencer (le plus simple)
2. **Gardez votre PC allum√©** quand vous voulez utiliser l'IA
3. **Utilisez un script** pour d√©marrer automatiquement
4. **Testez d'abord localement** avant de d√©ployer
5. **Surveillez les logs** Vercel pour les erreurs

## üöÄ **D√©ploiement**

1. **Configurer le tunnel** sur votre PC
2. **Ajouter la variable d'environnement** sur Vercel
3. **Red√©ployer** l'application
4. **Tester** l'IA sur Vercel

Votre Ollama local sera maintenant accessible depuis Vercel ! üéâ
