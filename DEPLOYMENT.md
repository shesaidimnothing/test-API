# Guide de D√©ploiement

## üöÄ **D√©ploiement sur Vercel (Mode D√©mo)**

L'application est configur√©e pour fonctionner sur Vercel en mode d√©mo sans Ollama.

### Fonctionnalit√©s en Mode D√©mo
- ‚úÖ Interface de chat compl√®te
- ‚úÖ Coloration syntaxique du code
- ‚úÖ Formatage markdown
- ‚úÖ Animations fluides
- ‚úÖ R√©ponses pr√©-programm√©es intelligentes

### D√©ploiement
1. **Connectez votre repository √† Vercel**
2. **Configurez les variables d'environnement** (optionnel) :
   ```
   NEXT_PUBLIC_DEFAULT_MODEL=llama2
   ```
3. **D√©ployez** - Vercel d√©tectera automatiquement le mode d√©mo

## üè† **D√©ploiement Local (Mode Complet)**

Pour utiliser l'IA Ollama compl√®te :

### Pr√©requis
- Node.js 18+
- Ollama install√© localement

### Installation
```bash
# 1. Cloner le repository
git clone <votre-repo>
cd prompt-project

# 2. Installer les d√©pendances
npm install

# 3. Installer et d√©marrer Ollama
# T√©l√©charger depuis https://ollama.ai
ollama serve

# 4. T√©l√©charger un mod√®le
ollama pull llama2
# ou
ollama pull GandalfBaum/deepseek_r1-claude3.7:latest

# 5. D√©marrer l'application
npm run dev
```

### Configuration Ollama
```bash
# V√©rifier les mod√®les install√©s
ollama list

# Tester un mod√®le
ollama run llama2

# V√©rifier que l'API fonctionne
curl http://127.0.0.1:11434/api/tags
```

## üîß **Variables d'Environnement**

### Vercel (Optionnel)
```env
NEXT_PUBLIC_DEFAULT_MODEL=llama2
NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:11434
```

### Local
```env
OLLAMA_HOST=http://127.0.0.1:11434
NEXT_PUBLIC_DEFAULT_MODEL=GandalfBaum/deepseek_r1-claude3.7:latest
```

## üéØ **Modes de Fonctionnement**

### Mode D√©mo (Vercel)
- D√©tection automatique en production
- R√©ponses pr√©-programm√©es intelligentes
- Interface compl√®te sans IA
- Parfait pour la d√©monstration

### Mode Complet (Local)
- IA Ollama compl√®te
- Tous les mod√®les disponibles
- R√©ponses en temps r√©el
- D√©veloppement et utilisation

## üêõ **D√©pannage**

### Erreur 500 sur Vercel
- **Cause** : Ollama n'est pas disponible sur Vercel
- **Solution** : L'application bascule automatiquement en mode d√©mo

### Ollama ne r√©pond pas localement
```bash
# V√©rifier que Ollama est d√©marr√©
ollama serve

# V√©rifier les mod√®les
ollama list

# Tester l'API
curl http://127.0.0.1:11434/api/tags
```

### Mod√®le non trouv√©
```bash
# Installer le mod√®le
ollama pull <nom-du-mod√®le>

# V√©rifier l'installation
ollama list
```

## üì± **Fonctionnalit√©s**

### Interface
- Chat en temps r√©el
- Messages avec avatars
- Scroll automatique
- Animations fluides

### Code
- Coloration syntaxique
- Support multi-langages
- Num√©rotation des lignes
- Bouton de copie

### Formatage
- Titres (# ## ###)
- Listes (- * + 1. 2. 3.)
- Citations (>)
- Liens [texte](url)
- Code inline `code`

## üöÄ **Prochaines √âtapes**

1. **Testez l'application** en mode d√©mo sur Vercel
2. **Clonez le repository** pour le mode complet
3. **Installez Ollama** pour l'IA compl√®te
4. **Personnalisez** selon vos besoins

L'application est maintenant pr√™te pour la production ! üéâ
